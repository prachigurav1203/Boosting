{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.**\n",
        "\n",
        "Answer:\n",
        "Boosting is an ensemble learning technique in machine learning that aims to create a strong predictive model by combining multiple weak learners—models that perform slightly better than random guessing. Unlike Bagging, where models are trained independently, Boosting trains models sequentially, with each new model focusing on the errors made by the previous models. By giving more weight to data points that were misclassified or poorly predicted, Boosting ensures that the ensemble progressively improves its accuracy.\n",
        "\n",
        "The process begins with a weak learner trained on the original dataset. The errors or misclassifications from this model are analyzed, and the next weak learner is trained to correct those mistakes. This sequential correction continues for a specified number of iterations or until the error is minimized. The final prediction is made by combining all weak learners, often using a weighted majority vote for classification or weighted averaging for regression.\n",
        "\n",
        "Boosting improves weak learners by emphasizing difficult cases, reducing both bias and error in the overall model. Popular Boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost, all of which implement this principle in slightly different ways. In summary, Boosting transforms a collection of individually weak models into a highly accurate, robust ensemble by iteratively focusing on the mistakes of prior learners and combining their predictions strategically."
      ],
      "metadata": {
        "id": "ym86KStZXvCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?**\n",
        "\n",
        "Answer:\n",
        "AdaBoost (Adaptive Boosting) and Gradient Boosting are both ensemble learning techniques based on the boosting principle, but they differ in how they train sequential models and handle errors.\n",
        "\n",
        "In AdaBoost, each weak learner is trained sequentially, and the training process focuses on misclassified samples from previous learners. Specifically, data points that were incorrectly predicted in earlier iterations are assigned higher weights so that subsequent models pay more attention to them. The final prediction is made by combining all weak learners using a weighted majority vote, where the weights are based on each learner’s accuracy. AdaBoost emphasizes correcting classification errors by dynamically adjusting sample weights, which directly impacts the training of the next model.\n",
        "\n",
        "In Gradient Boosting, the sequential training process is based on gradient descent optimization. Instead of reweighting samples, Gradient Boosting trains each new model to predict the residual errors (the difference between the actual target and the prediction) of the previous model. This approach reduces the overall loss function step by step, effectively minimizing prediction errors. Each new model learns to correct the mistakes of the ensemble in terms of the residuals, and the final prediction is obtained by adding up the outputs of all models, often scaled by a learning rate to control overfitting.\n",
        "\n",
        "In summary, the key difference is that AdaBoost adjusts sample weights to focus on misclassified instances, while Gradient Boosting fits each new model to the residual errors of the previous models using gradient descent on a loss function. Both methods sequentially improve weak learners, but Gradient Boosting is more flexible because it can optimize any differentiable loss function, making it suitable for both regression and classification tasks."
      ],
      "metadata": {
        "id": "eqyT3dwbXu-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: How does regularization help in XGBoost?**\n",
        "\n",
        "Answer:\n",
        "XGBoost (Extreme Gradient Boosting) is a powerful boosting algorithm widely used in machine learning competitions and real-world applications due to its high accuracy and flexibility. One of the key reasons for its success is the incorporation of regularization techniques, which help prevent overfitting and improve the generalization ability of the model. Regularization in XGBoost works by adding a penalty term to the objective function, which the algorithm minimizes during training. This penalty discourages overly complex models and ensures that the predictions remain robust on unseen data.\n",
        "\n",
        "In more detail, the XGBoost objective function consists of two parts: the training loss (which measures how well the model fits the data) and the regularization term (which penalizes model complexity). The regularization term typically includes two components: the L1 regularization (also known as Lasso) and the L2 regularization (also known as Ridge). L1 regularization penalizes the absolute values of leaf weights, promoting sparsity and effectively performing feature selection. L2 regularization penalizes the squared values of leaf weights, which stabilizes learning and prevents excessively large weight values that could lead to overfitting.\n",
        "\n",
        "Regularization in XGBoost helps in several important ways:\n",
        "\n",
        "Reduces Overfitting: By penalizing complex trees with too many splits or extreme leaf weights, regularization prevents the model from fitting noise in the training data, ensuring better performance on new, unseen data.\n",
        "\n",
        "Controls Model Complexity: Regularization parameters such as gamma (minimum loss reduction required to make a further split) and lambda/alpha (L2/L1 penalties) control the growth of trees and the magnitude of leaf weights. This ensures that each tree contributes meaningfully without being excessively complex.\n",
        "\n",
        "Improves Generalization: By balancing the trade-off between fitting the training data and keeping the model simple, regularization helps the model generalize better, which is critical in real-world applications where data distribution may vary.\n",
        "\n",
        "Stabilizes Optimization: Regularization reduces the impact of noisy gradients, leading to smoother convergence during the boosting process and making training more stable and reliable.\n",
        "\n",
        "In practical terms, when using XGBoost, tuning regularization parameters is essential. For example, increasing lambda (L2 regularization) or alpha (L1 regularization) can help reduce overfitting on datasets with many features or noisy data. Similarly, adjusting gamma ensures that only splits that significantly reduce the loss are made, avoiding unnecessary complexity.\n",
        "\n",
        "Summary:\n",
        "Regularization in XGBoost is a critical feature that enhances the robustness and predictive power of the model. By penalizing overly complex trees and extreme leaf weights, it prevents overfitting, controls model complexity, improves generalization, and stabilizes training. This allows XGBoost to deliver highly accurate predictions in a wide range of tasks, from classification and regression to ranking problems, while remaining reliable and interpretable."
      ],
      "metadata": {
        "id": "R8pIg5dCXu7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: Why is CatBoost considered efficient for handling categorical data?**\n",
        "\n",
        "Answer:\n",
        "CatBoost, short for Categorical Boosting, is a gradient boosting algorithm designed to efficiently handle categorical features in datasets without the need for extensive preprocessing like one-hot encoding. Many machine learning algorithms struggle with categorical data, especially when there are high-cardinality features (features with many unique values), because traditional encoding methods can dramatically increase dimensionality, leading to overfitting, increased computation, and reduced performance. CatBoost addresses these challenges with innovative techniques that make it highly efficient for datasets containing categorical variables.\n",
        "\n",
        "The core reason CatBoost is efficient lies in its ordered target encoding method. Instead of naively converting categorical values into numbers based on their frequency or target averages, which can lead to target leakage, CatBoost computes statistics for each categorical feature in an ordered and incremental manner. During training, for a given data point, the algorithm only uses information from preceding rows to encode its categorical features, avoiding the use of future information. This ensures that the model does not “cheat” by learning from the target variable in ways that would inflate accuracy on the training set but fail in real-world predictions.\n",
        "\n",
        "Additionally, CatBoost uses efficient handling of high-cardinality features. Traditional one-hot encoding creates many sparse columns, which can be computationally expensive and memory-intensive. CatBoost avoids this by encoding categorical variables into numerical representations that preserve their relationship with the target, allowing the boosting process to handle them directly without significant dimensionality expansion.\n",
        "\n",
        "Other factors contributing to CatBoost’s efficiency include:\n",
        "\n",
        "Ordered Boosting: CatBoost implements a special variant of gradient boosting called ordered boosting, which reduces overfitting, especially on small datasets or datasets with categorical features.\n",
        "\n",
        "Automatic Handling of Categorical Features: Users do not need to manually encode categorical variables; the algorithm automatically identifies and processes them during training.\n",
        "\n",
        "Reduced Overfitting: By avoiding naive target encoding and using ordered statistics, CatBoost minimizes the risk of overfitting that is common in high-cardinality categorical data.\n",
        "\n",
        "Speed and Scalability: CatBoost uses optimized GPU and CPU implementations, making it capable of handling large datasets efficiently while preserving accuracy.\n",
        "\n",
        "Summary:\n",
        "CatBoost is considered efficient for handling categorical data because it automatically encodes categorical features in a statistically sound manner, avoids target leakage through ordered target encoding, reduces overfitting via ordered boosting, and can handle high-cardinality variables without creating a large number of sparse features. This makes CatBoost particularly effective in real-world datasets that contain a mix of numerical and categorical data, such as financial, retail, or customer behavior datasets, where preprocessing can be complex and error-prone."
      ],
      "metadata": {
        "id": "zOQV6SF1Xu3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?**\n",
        "\n",
        "Answer:\n",
        "Boosting techniques, such as AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost, are preferred over bagging methods in real-world applications where high predictive accuracy is critical and the data may have complex patterns or class imbalances. Unlike bagging, which primarily reduces variance by training multiple independent models on bootstrapped samples, boosting focuses on sequentially correcting errors of weak learners, thereby reducing both bias and variance. This makes boosting particularly effective for problems that require precision in prediction, especially when misclassification carries a significant cost.\n",
        "\n",
        "Some real-world applications where boosting techniques are preferred include:\n",
        "\n",
        "Financial Risk and Credit Scoring:\n",
        "In banking and insurance, predicting loan defaults, credit card fraud, or insurance claims requires high accuracy because errors can result in financial losses. Boosting algorithms excel in these scenarios by focusing on difficult-to-predict cases, improving the identification of high-risk customers.\n",
        "\n",
        "Healthcare and Medical Diagnosis:\n",
        "Boosting is used in predicting disease outcomes, patient readmission, or medical image classification. For instance, Gradient Boosting and XGBoost have been applied to detect cancer from medical scans or predict heart disease, where missing critical cases could have severe consequences.\n",
        "\n",
        "Customer Churn Prediction:\n",
        "Businesses use boosting techniques to predict which customers are likely to leave a service. Since churn datasets often have imbalanced classes (few customers churn vs. many stay), boosting’s focus on misclassified examples helps improve predictive performance on the minority class.\n",
        "\n",
        "E-commerce and Recommendation Systems:\n",
        "Boosting is applied to ranking products or predicting purchase behavior, where small improvements in accuracy can significantly increase revenue. Gradient Boosting and LightGBM are particularly effective in handling large-scale transactional datasets with complex feature interactions.\n",
        "\n",
        "Fraud Detection and Cybersecurity:\n",
        "In credit card transactions, network security, or online payment systems, the cost of false negatives (failing to detect fraud) is high. Boosting algorithms are preferred because they iteratively focus on difficult-to-detect fraudulent cases, improving detection rates even when fraudulent instances are rare.\n",
        "\n",
        "Predictive Maintenance and Industrial Applications:\n",
        "In manufacturing or IoT systems, boosting models can predict equipment failure or anomalies based on sensor data, which often has complex non-linear relationships. Sequential learning in boosting allows the model to capture subtle patterns in operational data that bagging might miss.\n",
        "\n",
        "Summary:\n",
        "Boosting techniques are preferred over bagging in applications where high accuracy, handling of difficult or minority cases, and capturing complex patterns are crucial. By sequentially correcting errors of weak learners, boosting produces strong models that are highly effective in finance, healthcare, fraud detection, marketing, and other critical domains. Bagging methods, while robust and less prone to overfitting, may not achieve the same level of predictive precision in these scenarios."
      ],
      "metadata": {
        "id": "iZ9jwsu4Xusa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets:\n",
        "● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "● Use sklearn.datasets.fetch_california_housing() for regression\n",
        "tasks.\n",
        "\n",
        "**Question 6: Write a Python program to:**\n",
        "\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Print the model accuracy\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "Bf81bEBXYjBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Step 2: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 3: Split dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Step 4: Define a weak learner (Decision Tree with max_depth=1)\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "\n",
        "# Step 5: Train an AdaBoost Classifier\n",
        "ada_model = AdaBoostClassifier(\n",
        "    estimator=base_estimator,   # base learner\n",
        "    n_estimators=50,            # number of weak learners\n",
        "    learning_rate=1.0,          # step size for boosting\n",
        "    random_state=42\n",
        ")\n",
        "ada_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict on the test set and calculate accuracy\n",
        "y_pred = ada_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of AdaBoost Classifier:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J3tdgmOYtaw",
        "outputId": "acbb5fc6-8aa8-4dad-d84d-24d9532cdb57"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of AdaBoost Classifier: 0.9590643274853801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to:**\n",
        "\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "● Evaluate performance using R-squared score\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "GDNX_jT2Yi97"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57I3-qGUXCxJ",
        "outputId": "0ed03f88-4657-4a69-f1f2-54a900b7d472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score of Gradient Boosting Regressor: 0.7803012822391022\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Step 2: Load the California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X = california.data\n",
        "y = california.target\n",
        "\n",
        "# Step 3: Split dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Define and train the Gradient Boosting Regressor\n",
        "gbr_model = GradientBoostingRegressor(\n",
        "    n_estimators=100,   # number of boosting stages\n",
        "    learning_rate=0.1,  # step size\n",
        "    max_depth=3,        # maximum depth of each tree\n",
        "    random_state=42\n",
        ")\n",
        "gbr_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict on the test set\n",
        "y_pred = gbr_model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-squared Score of Gradient Boosting Regressor:\", r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:**\n",
        "\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Tune the learning rate using GridSearchCV\n",
        "\n",
        "● Print the best parameters and accuracy\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "5pRELjuMY9fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Step 2: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 3: Split dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Step 4: Define the XGBoost Classifier\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Step 5: Define hyperparameter grid for learning_rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Step 6: Use GridSearchCV to find the best learning rate\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Get the best parameters and best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Step 8: Predict on the test set and evaluate accuracy\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 9: Print results\n",
        "print(\"Best Learning Rate:\", best_params['learning_rate'])\n",
        "print(\"Accuracy of XGBoost Classifier:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN204PSTY9Gi",
        "outputId": "756508a1-e1dc-46d2-cfb2-26d066553c65"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [04:20:05] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Learning Rate: 0.05\n",
            "Accuracy of XGBoost Classifier: 0.9590643274853801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:**\n",
        "\n",
        "● Train a CatBoost Classifier\n",
        "\n",
        "● Plot the confusion matrix using seaborn\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "D6cXPLJqZKJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2--Fp6rZeNJ",
        "outputId": "d16c556c-c1a9-49ca-c4c0-f9bc116c5b88"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize CatBoost Classifier\n",
        "cat_model = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=3,\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "cat_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = cat_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of CatBoost Classifier:\", accuracy)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "LWmnkji_Y7vI",
        "outputId": "8babe351-8dc2-4c06-9248-1bad3bb05a91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of CatBoost Classifier: 0.9532163742690059\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You're working for a FinTech company trying to predict loan default using ustomer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.**\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "Predicting loan default in a real-world FinTech scenario requires careful handling of imbalanced data, missing values, and mixed feature types. A boosting-based approach can provide highly accurate predictions while capturing complex patterns in the data. Here is a step-by-step pipeline:\n",
        "\n",
        "1. Data Preprocessing & Handling Missing/Categorical Values\n",
        "\n",
        "Missing Values: Use imputation strategies based on the feature type:\n",
        "\n",
        "Numerical features → SimpleImputer with median.\n",
        "\n",
        "Categorical features → SimpleImputer with mode.\n",
        "\n",
        "Categorical Features: Encode using techniques that integrate with boosting algorithms:\n",
        "\n",
        "For CatBoost, categorical features can be passed directly.\n",
        "\n",
        "For XGBoost/AdaBoost, apply OneHotEncoder or TargetEncoder.\n",
        "\n",
        "Feature Scaling: Boosting models like XGBoost and CatBoost are tree-based, so scaling is generally not required.\n",
        "\n",
        "Handling Imbalance: Use techniques like:\n",
        "\n",
        "class_weight='balanced' in XGBoost or AdaBoost.\n",
        "\n",
        "Oversampling the minority class using SMOTE or undersampling the majority class.\n",
        "\n",
        "2. Choice Between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "AdaBoost: Simple to implement but may struggle with high-dimensional or imbalanced data.\n",
        "\n",
        "XGBoost: Powerful and flexible; supports weighted classes and regularization to prevent overfitting.\n",
        "\n",
        "CatBoost: Highly efficient for datasets with categorical variables and handles missing values automatically.\n",
        "\n",
        "Recommendation: Use CatBoost for this scenario because it natively handles categorical features, missing values, and imbalanced classes without extensive preprocessing.\n",
        "\n",
        "3. Hyperparameter Tuning Strategy\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV for key hyperparameters:\n",
        "\n",
        "iterations / n_estimators → number of trees.\n",
        "\n",
        "learning_rate → step size for boosting.\n",
        "\n",
        "depth → maximum depth of trees.\n",
        "\n",
        "l2_leaf_reg → regularization to prevent overfitting.\n",
        "\n",
        "Use Stratified K-Fold cross-validation to preserve the imbalance distribution in each fold.\n",
        "\n",
        "4. Evaluation Metrics\n",
        "\n",
        "Given the imbalanced dataset, accuracy alone is insufficient. Use:\n",
        "\n",
        "Precision, Recall, F1-score: Measure how well defaulters are identified.\n",
        "\n",
        "ROC-AUC Score: Evaluates model discrimination ability.\n",
        "\n",
        "Confusion Matrix: Provides a clear understanding of true positives, false positives, etc.\n",
        "\n",
        "These metrics ensure the model focuses on minimizing financial risk by correctly identifying high-risk customers.\n",
        "\n",
        "5. Business Benefits\n",
        "\n",
        "Risk Reduction: Predicting potential defaulters helps the company reduce financial losses.\n",
        "\n",
        "Targeted Interventions: High-risk customers can receive tailored repayment plans or credit limits.\n",
        "\n",
        "Efficient Resource Allocation: Focus collections and risk management efforts on customers likely to default.\n",
        "\n",
        "Data-Driven Decision Making: Feature importance from CatBoost provides insights into key risk factors like transaction patterns or credit history."
      ],
      "metadata": {
        "id": "oewORGSWZnMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install CatBoost if not already installed\n",
        "!pip install catboost\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Step 3: Load dataset (example: using a CSV or synthetic dataset)\n",
        "# df = pd.read_csv('loan_data.csv')\n",
        "# For demonstration, we create a synthetic dataset\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=10, n_informative=6, n_redundant=2,\n",
        "    n_classes=2, weights=[0.8, 0.2], flip_y=0.01, random_state=42\n",
        ")\n",
        "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
        "df['target'] = y\n",
        "\n",
        "# Step 4: Introduce missing values for demonstration\n",
        "import numpy as np\n",
        "df.loc[df.sample(frac=0.1).index, 'feature_0'] = np.nan\n",
        "\n",
        "# Step 5: Split features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Step 6: Impute missing values\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Step 7: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_imputed, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 8: Initialize and train CatBoost Classifier\n",
        "cat_model = CatBoostClassifier(\n",
        "    iterations=200,\n",
        "    learning_rate=0.1,\n",
        "    depth=4,\n",
        "    eval_metric='AUC',\n",
        "    random_state=42,\n",
        "    verbose=0,\n",
        "    class_weights=[1, 4]  # handle imbalance\n",
        ")\n",
        "cat_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Predictions and evaluation\n",
        "y_pred = cat_model.predict(X_test)\n",
        "y_prob = cat_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b_XfXqAZm43",
        "outputId": "94af3505-bddf-488c-b7fd-5cf3b892f6df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95       239\n",
            "           1       0.80      0.80      0.80        61\n",
            "\n",
            "    accuracy                           0.92       300\n",
            "   macro avg       0.88      0.88      0.88       300\n",
            "weighted avg       0.92      0.92      0.92       300\n",
            "\n",
            "Confusion Matrix:\n",
            "[[227  12]\n",
            " [ 12  49]]\n",
            "ROC-AUC Score: 0.9628232389052747\n"
          ]
        }
      ]
    }
  ]
}